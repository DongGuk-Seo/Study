{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  class  \n",
       "564                0.2216          0.2060                  0.07115      0  \n",
       "565                0.1628          0.2572                  0.06637      0  \n",
       "566                0.1418          0.2218                  0.07820      0  \n",
       "567                0.2650          0.4087                  0.12400      0  \n",
       "568                0.0000          0.2871                  0.07039      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['class'] = cancer.target\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension',\n",
       "       'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 31])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.from_numpy(df.values).float()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([569, 10]) torch.Size([569, 1])\n"
     ]
    }
   ],
   "source": [
    "x = data[:, :10]\n",
    "y = data[:,-1:]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [.6, .2, .2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 341, valid : 113, test : 115\n"
     ]
    }
   ],
   "source": [
    "train_cnt = int(data.size(0) * ratios[0])\n",
    "valid_cnt = int(data.size(0) * ratios[1])\n",
    "test_cnt = data.size(0) - train_cnt - valid_cnt\n",
    "cnts = [train_cnt, valid_cnt, test_cnt]\n",
    "print(f'Train : {train_cnt}, valid : {valid_cnt}, test : {test_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([341, 10]) torch.Size([341, 1])\n",
      "torch.Size([113, 10]) torch.Size([113, 1])\n",
      "torch.Size([115, 10]) torch.Size([115, 1])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.randperm(data.size(0))\n",
    "\n",
    "x = torch.index_select(x, dim=0, index=indices)\n",
    "y = torch.index_select(y, dim=0, index=indices)\n",
    "\n",
    "x = x.split(cnts, dim=0)\n",
    "y = y.split(cnts, dim=0)\n",
    "\n",
    "for x_i,y_i in zip(x,y):\n",
    "    print(x_i.size(),y_i.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "batch_size = 128\n",
    "print_interval = 500\n",
    "early_stop = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train : 341, Valid : 113, Test : 115\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=CustomDataset(x[0],y[0]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, # Only training sets need to be shuffled\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=CustomDataset(x[1],y[1]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=CustomDataset(x[2],y[2]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\" Train : {len(train_loader.dataset)}, Valid : {len(valid_loader.dataset)}, Test : {len(test_loader.dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=6, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Linear(in_features=6, out_features=5, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01)\n",
       "  (4): Linear(in_features=5, out_features=4, bias=True)\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (7): LeakyReLU(negative_slope=0.01)\n",
       "  (8): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (9): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(x[0].size(-1), 6),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(6,5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5,4),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(4,3),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(3,y[0].size(-1)),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "lowest_loss = np.inf\n",
    "best_model = None\n",
    "\n",
    "lowest_epoch = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: train loss=2.559942e-01 valid_loss=2.624939e-01 lowest_loss=2.596960e-01\n",
      "Epoch 1000: train loss=2.339345e-01 valid_loss=2.518721e-01 lowest_loss=2.325241e-01\n",
      "Epoch 1500: train loss=2.023879e-01 valid_loss=2.061983e-01 lowest_loss=1.918580e-01\n",
      "Epoch 2000: train loss=1.658099e-01 valid_loss=1.451418e-01 lowest_loss=1.442778e-01\n",
      "Epoch 2500: train loss=1.656502e-01 valid_loss=1.323239e-01 lowest_loss=1.314726e-01\n",
      "There is no improvement during last 100 epoch\n",
      "The best validation loss from epoch 2616 : 1.3084e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history, valid_history = [], []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss, valid_loss = 0, 0\n",
    "    y_hat = []\n",
    "\n",
    "    for x_i,y_i in train_loader:\n",
    "        y_hat_i = model(x_i)\n",
    "        loss = F.binary_cross_entropy(y_hat_i,y_i)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += float(loss)\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "\n",
    "        for x_i,y_i in valid_loader:\n",
    "            y_hat_i = model(x_i)\n",
    "            loss = F.binary_cross_entropy(y_hat_i,y_i)\n",
    "\n",
    "            valid_loss += float(loss)\n",
    "\n",
    "            y_hat += [y_hat_i]\n",
    "    \n",
    "    valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "    train_history += [train_loss]\n",
    "    valid_history += [valid_loss]\n",
    "\n",
    "    if (i + 1) % print_interval == 0:\n",
    "        print('Epoch %d: train loss=%4e valid_loss=%4e lowest_loss=%4e'% (\n",
    "            i + 1,\n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            lowest_loss,\n",
    "        ))\n",
    "    if valid_loss <= lowest_loss:\n",
    "        lowest_loss = valid_loss\n",
    "        lowest_epoch = i\n",
    "\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "    else:\n",
    "        if early_stop > 0 and lowest_epoch + early_stop < i + 1:\n",
    "            print('There is no improvement during last %d epoch' % early_stop)\n",
    "            break\n",
    "    \n",
    "print('The best validation loss from epoch %d : %.4e' % (lowest_epoch + 1, lowest_loss))\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "093657fb88d3298cb6dc64534140a102cc63ad41041863c9d9a446080cf338c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
