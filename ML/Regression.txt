Linear Regression :y = Wx + b로 표시되는 선형식으로 x와 y의 관계를 찾는 모델 (전체 데이터의 경향성을 파악하는 선형식을 구하는 방법)
 - 특징 :
  1. 분류와 다르게, 회귀 모델은 계산 결과 자체가 예측 값
  2. 특정 값을 예측하는 것을 목표로 설정 (특정 카테고리 예측에는 적합하지 않음)
  3. 특정 값을 찾기 위해 주어진 데이터의 패턴을 탐구
  4. 패턴을 선형식으로 파악하는 것이 Linear Regression
 - 가장 많이 사용하는 Loss function은 MSE(Mean Squared Error)
  1. 모델의 예측값이 실제값에 점점 가까워지게 학습 -> 전체적으로 Loss의 평균이 작아지는 방향으로 학습
  2. 차이가 큰 데이터가 있는 경우 Loss가 더 크게 나오기 때문에 outlier 데이터가 있으면 미리 제거 또는 보정이 필요
  3. Gradient descent algorithm이 사용되어 w, b를 업데이트
 - Linear Regression은 파라미터 W와 b를 찾는 문제
 - 장점 :
  1. 통계적으로 설명 가능한 이론이 많다(설명 도구가 많다)
  2. interpretability가 있다(설명 가능하다)
  3. Linear model 자체가 가지는 simplicity 때문에, general한 모델이 나오는 편(복잡한 모델보다 때로 예측력이 더 뛰어남)

Lasso, Ridge : Linear Regression 모델이 고차원 공간에 overfitting이 쉽게 되는 문제를 해결한 기법
 - Simple Linear Regression : y = Wx + b, MSE function
 - Lasso : 
  1. weight의 L1 term을 Loss function에 더해줍니다(ʎ는 hyper-parameter)
  2. Loss가 무조건 증가
  3. 추가한 항(L1 term)도 Gradient descent algorithm의 최적화 대상
  4. L1 term을 제약조건이라 부르고 Regularization term 이라고도 함 (L1 Regularization term)
 - Ridge :
  1. weight의 L2 term을 Loss function에 더해줍니다(ʎ는 hyper-parameter)
  2. Loss가 무조건 증가
  3. 추가한 항(L2 term)도 Gradient descent algorithm의 최적화 대상
  4. L2 term을 제약조건이라 부르고 Regularization term 이라고도 함 (L2 Regularization term)
 - Lasso나 Ridge를 적용했을 때, 성능이 향상된다면 Linear Regression 모델에 사용되는 feature vector가 차원을 줄일 필요가 있다는 얘기(feature selection이 성능 향상을 가져옴)
 - Regularization을 할 때, weight를 사용하는 방식을 weight decay라고 함
 - weight decay를 주게 되면, Gradient descent algorithm이 loss space를 탐색할 때 제약 조건을 받게되는 효과가 있음
 - 제약 조건 때문에, 특정 weight들이 사라지는 효과가 생기면서(0에 가까워짐) feature subset selection을 하는 효과가 있음

XGBoost : 하드웨어 최적화를 시킨 Gradient Boosting Model
 - Gradient Boosting Model 특징 :
  1. Boosting model은 Bagging 방식이 만들어지는 원리가 전체 성능을 향상하는데 직접적인 연관이 없는 것을 보완한 모델
  2. Sequential model
  3. 첫 번째로 만든 Desicion Tree가 잘못 분류한 자료들을 그 다음 Desicion Tree가 보완하는 방식으로 순차적으로 Tree를 build
  4. 다음 Desicion Tree는 이전 Desicion Tree가 잘못 분류한 데이터들에 weight를 주는 것으로 뽑을 데이터의 sampling을 조절
  5. Gradient descent algorithm을 Boosting model에 도입해서 다음 DT가 이전 DT와 합쳐져서 더 적은 loss를 가지게 되는 방향으로 DT를 만드는 방법
 - XGBoost 원리 :
  1. Gradient Boosting model + System Optimization
  2. Tree의 best split point를 찾을 때, feature를 정렬하는게 가장 큰 cost를 소모한다는 점을 확인
  3. 정렬하는 비용을 block 단위로 잘라서 update하는 방법을 제안하여 GBM과 거의 유사한 성능을 내는 방식을 제안
  4. 훨씬 더 빠르게 정렬한 내용들을 사용할 수 있게 시스템 최적화를 사용
  5. GPU Acceleration, Cache awareness, I/O performance를 개선하여 훨씬 더 빠르게 학습이 가능(C++로 구현)
 - Recap : 
  1. XGBoost는 시스템 최적화를 통해서 practical한 좋은 솔루션을 제안
  2. GPU를 사용할 수 있게 되어서, computing resource를 이전보다 더 많이 사용해서 더 좋은 성능을 낼 수있는 방법을 제안


LightGBM : 기존 GBM보다 속도 향상이 된 모델
 - 기존 모델의 특징 : 
  1. 기존 GBM은 Level-wise 방식으로 Tree를 build(Level-wise는 DT가 학습할 때, 같은 level의 노드들을 모두 split 후 다음 level로 넘어가는 방식)
  2. 깊이가 너무 커지면 overfitting 가능성이 높아져 level을 제한하여 최대한 모들을 키우는 방식을 사용(Model Generalization)
 - LGBM 특징 :
  1. LGBM의 메인 아이디어는 Level-wise/Leaf-wise 방식 모두 optimal을 만들게 된다면 비슷한 DT를 만들게 된다는 것에서 시작
  2. Leaf-wise를 사용한다면 훨씬 더 빠르게 optimal을 찾을 수 있다는 것이 포인트
  3. 전체 Loss가 줄어드는 방향으로 node를 선정해서 split을 진행(이 때 level을 유지하려는 경향을 포기)
  4. 필요 node만 split하면 되기 때문에, 기존 GBM보다 훨씬 빠르게 학습이 가능
  5. 데이터가 적으면 overfitting이 될 가능성이 상승 (10,000 rows 이상일 때 권장)
  6. 다른 GBM들에 비해 hyper-parameter sensitive (특히 max_depth에 가장 민감)